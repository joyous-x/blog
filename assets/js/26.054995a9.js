(window.webpackJsonp=window.webpackJsonp||[]).push([[26],{479:function(t,a,e){t.exports=e.p+"assets/img/image-to-image_objective.0580bae8.png"},687:function(t,a,e){"use strict";e.r(a);var r=e(15),i=Object(r.a)({},(function(){var t=this,a=t.$createElement,r=t._self._c||a;return r("ContentSlotsDistributor",{attrs:{"slot-key":t.$parent.slotKey}},[r("h1",{attrs:{id:"论文阅读摘要"}},[r("a",{staticClass:"header-anchor",attrs:{href:"#论文阅读摘要"}},[t._v("#")]),t._v(" 论文阅读摘要")]),t._v(" "),r("h1",{attrs:{id:"image-to-image-translation-with-conditional-adversarial-networks"}},[r("a",{staticClass:"header-anchor",attrs:{href:"#image-to-image-translation-with-conditional-adversarial-networks"}},[t._v("#")]),t._v(" Image-to-Image Translation with Conditional Adversarial Networks")]),t._v(" "),r("h2",{attrs:{id:"abstract"}},[r("a",{staticClass:"header-anchor",attrs:{href:"#abstract"}},[t._v("#")]),t._v(" Abstract")]),t._v(" "),r("p",[t._v("针对 转化输入图像到一种关联的输出(image-to-image, map pixels to pixels)类型的任务，提出了一种通用的 architecture 和 objective，只是使用不同的数据即可满足各种任务")]),t._v(" "),r("p",[t._v("一般来说，Conditional adversarial nets 在这类任务中有不俗的表现。这些网络不仅学习到了 输入到输出 的映射，还学习到了一个用于训练这个映射的 loss function。另外，我们证明了：这种方法在以下任务中都是卓有成效的：从 (semantic) label maps 合成照片、从图像轮廓重构完整图形、图像着色，以及其他的类似任务。而且，不用进行 parameters、loss function 的调整就可以达到比较合理的结果。")]),t._v(" "),r("h2",{attrs:{id:"_1-introduction"}},[r("a",{staticClass:"header-anchor",attrs:{href:"#_1-introduction"}},[t._v("#")]),t._v(" 1. Introduction")]),t._v(" "),r("p",[t._v("使用 CNNs 以及针对性设计的 loss，也可以达到不错的效果。但是这要求精心的设计损失函数。如果简单的使用输入、输出的欧式距离作为损失函数，会大概率得到比较模糊的结果：这是因为为了使欧式距离最小化，网络会更倾向于将合理的输出平均化，这会造成模糊。")]),t._v(" "),r("p",[t._v("一般的，我们一些专业知识才能够设计出合理的损失函数，以使得 CNNs 可以有效的收敛出符合我们预期的结果。")]),t._v(" "),r("p",[t._v("幸运的事，通过 GANs，我们可以指定更高纬度的目标(如，使输出与现实无法区分)，就能自主学习出适合于所用数据、所期目标的 loss 函数。也就是说，GANs 可以通过使用不同的数据，自主学习到满足我们需要的损失函数。这在以前是需要使用不同的损失函数才能做到的。")]),t._v(" "),r("p",[t._v("本论文的主要贡献有：")]),t._v(" "),r("ol",[r("li",[t._v("cGANs 可以对多种 image-to-image 问题产生合理的结果")]),t._v(" "),r("li",[t._v("提出一个简单并且足以取得良好的结果的框架，并分析几种重要的体系结构的效果。"),r("a",{attrs:{href:"https://github.com/phillipi/pix2pix",target:"_blank",rel:"noopener noreferrer"}},[t._v("Code is available"),r("OutboundLink")],1)])]),t._v(" "),r("h2",{attrs:{id:"_2-related-work"}},[r("a",{staticClass:"header-anchor",attrs:{href:"#_2-related-work"}},[t._v("#")]),t._v(" 2. Related Work")]),t._v(" "),r("p",[r("strong",[t._v("Structured losses for image modeling")]),t._v(' 针对 image-to-image 转换问题建模的损失函数，通常被表达成像素级(per-pixel)的分类或回归方程式。这些方程式认为解空间是"unstructured"，也就是说每个输出的pixel被认为在条件上独立于给定输入图像的所有其他像素。')]),t._v(" "),r("p",[t._v("相反，cGANs会学习结构性损失。结构化的损失会惩罚输出的联合配置。理论上，cGANs的损失函数可以对输入和输出间的任何不同结构进行惩罚。")]),t._v(" "),r("p",[t._v('事实上，在此之前已经有很多使用 cGANSs 来解决图像生成问题。本文与他们的几个不同的地方在于 generator 和 discriminator 的框架选择上。我们的生成器使用 "U-Net"-based 框架，评判器使用了卷积网络的分离器 "PatchGAN"（它只惩罚 patch 规模的图像结构）。')]),t._v(" "),r("h2",{attrs:{id:"_3-method"}},[r("a",{staticClass:"header-anchor",attrs:{href:"#_3-method"}},[t._v("#")]),t._v(" 3. Method")]),t._v(" "),r("p",[t._v("GANs are generative models that learn a mapping from\nrandom noise vector z to output image y, G : z → y [24]. In\ncontrast, conditional GANs learn a mapping from observed\nimage x and random noise vector z, to y, G : {x, z} → y")]),t._v(" "),r("h3",{attrs:{id:"_3-1-objective"}},[r("a",{staticClass:"header-anchor",attrs:{href:"#_3-1-objective"}},[t._v("#")]),t._v(" 3.1 Objective")]),t._v(" "),r("p",[r("img",{attrs:{src:e(479),alt:"Objective"}})]),t._v(" "),r("p",[t._v("这里需要注意的是 z 是必须的。以往的 cGANs 使用已经意识到这点，并且通常采用 Gaussian noise 作为 z 来微调输入。")]),t._v(" "),r("p",[t._v("在最初的实验中，我们使用了这种策略，但没有效果，生成器好像忽略了这个噪音。相反，对于最终模型，我们仅以 dropout 的形式提供噪音，并应用于\n训练和测试时的生成器的几层。")]),t._v(" "),r("p",[t._v("尽管采用了 dropout 噪声，我们观察到我们的网络输出的随机性依然很小。设计可产生更大随机性的条件GAN，从而捕获他们建模的条件分布的完全熵，是当前工作悬而未决的重要问题。")]),t._v(" "),r("h3",{attrs:{id:"_3-2-network-architectures"}},[r("a",{staticClass:"header-anchor",attrs:{href:"#_3-2-network-architectures"}},[t._v("#")]),t._v(" 3.2 Network architectures")]),t._v(" "),r("ol",[r("li",[r("strong",[t._v("Generator with skips")])])]),t._v(" "),r("p",[t._v("一个 image-to-image 转换问题的定义特征是：他们将高分辨率的输入网格映射到高分辨率的输出网格。此外，对于我们考虑的问题，输入和输出的表面外观不同，但是两者是相同基础结构的效果图。所以，输入与输出中的结构是大致对齐的。我们围绕这些事项来设计生成架构。")]),t._v(" "),r("p",[t._v("一些以前解决这类问题的方法是使用 encoder-decoder network。这种网络要求所有的信息流需要通过所有的层，包括瓶颈层。对于许多图像翻译问题，有很多底层信息在输入和输出之间共享，就是说，希望将这些信息直接穿梭在网络上。例如，在图像着色的情况下，输入和输出共享显著边缘的位置，")]),t._v(" "),r("p",[t._v("为了给生成器一种避免此类信息瓶颈的方法，我们按照“ U-Net”的一般形状添加了跳过连接。具体来说，我们在第i层和第n-i层之间添加跳过连接，其中n是层的总数。每个跳过连接仅将第i层和第n-i层的所有通道连接在一起。")]),t._v(" "),r("ol",{attrs:{start:"2"}},[r("li",[r("strong",[t._v("Markovian discriminator (PatchGAN)")])])]),t._v(" "),r("p",[t._v("众所周知，L2 和 L1 loss 在图像生成任务中会产生模糊结果。尽管这些损失并不能鼓励高频率清晰度，尽管如此，它们在许多情况下仍能准确捕获低频信息。对于这里的问题，确实是这样，我们不需要全新的框架来强化低频信息的正确性，L1 已经做了。")]),t._v(" "),r("p",[t._v("依靠 L1 来强制低频正确性，这促使将 GAN 判别器限制为仅对高频结构信息进行建模。为了对高频信息建模，将注意力集中在本地图像 patch 中的结构就足够了。因此，我们设计了被称为 PatchGAN 的判别架构，它仅对 patch 规模的结构进行惩罚。这个判别器尝试去分类图像中的 N * N 个 patch 是真是假。我们对该图像进行卷积运算，对所有响应求平均值以提供 D 的最终输出。")]),t._v(" "),r("p",[t._v("下文中，我们证明了：N 可以比图像小很多，也依然可以生成高质量的结果。这是有利的，因为较小的 PatchGAN 具有较少的参数，运行速度更快，并且可以适用于任意大图像。这样的鉴别器可以有效地将图像建模为马尔可夫随机场，假设像素之间的独立性相差大于 patch 直径。")]),t._v(" "),r("h3",{attrs:{id:"_3-3-optimization-and-inference"}},[r("a",{staticClass:"header-anchor",attrs:{href:"#_3-3-optimization-and-inference"}},[t._v("#")]),t._v(" 3.3 Optimization and inference")]),t._v(" "),r("p",[t._v("了优化我们的网络，我们遵循 GAN 中的标准方法：在 D 的一个梯度下降步骤与 G 的一个步骤之间交替。如原始 GAN 论文建议的那样，不是训练 G 以使 log(1 − D(x，G(x，z)) 最小化，而是训练以最大化 log(D(x，G(x，z))。此外，我们在优化 D 的同时将目标除以2，这会降低 D 相对于 G 的学习率。我们使用 minibatch SGD 和 应用 Adam 求解器，指定参数：学习率=0.0002、β1=0.5, β2=0.999。")]),t._v(" "),r("p",[t._v("在推理时，我们以和训练阶段完全相同的方式运行 生成器网络。这和通常的协议不太一样，在测试阶段，我们使用了 dropout 与训练阶段相同的方式，并且我们使用测试 batch 的统计信息进行 batch normalization，而不是训练批次的汇总统计信息。这种方法进行 batch normalization 时，设置 batch size 为 1 时被称为“实例归一化”，并已证明在图像生成任务中有效。在我们的实验中，我们使用 batch size 介于1到10之间，具体取决于实验。")]),t._v(" "),r("h2",{attrs:{id:"_4-experiments"}},[r("a",{staticClass:"header-anchor",attrs:{href:"#_4-experiments"}},[t._v("#")]),t._v(" 4. Experiments")]),t._v(" "),r("p",[t._v("为了探索 cGANs 的普遍性，我们在各种任务和数据集上测试了这种方法，包括 图形任务，例如照片生成，和视觉任务，例如语义分割：")]),t._v(" "),r("ol",[r("li",[t._v("Semantic labels ↔ photoo, trained on the Cityscapes dataset")]),t._v(" "),r("li",[t._v("Architectural labels → photo, trained on CMP Facades")]),t._v(" "),r("li",[t._v("Map↔aerial photo, trained on data scraped from Google Maps")]),t._v(" "),r("li",[t._v("BW → color photos")]),t._v(" "),r("li",[t._v("Edges → photo")]),t._v(" "),r("li",[t._v("Sketch → photo")]),t._v(" "),r("li",[t._v("Day → night")]),t._v(" "),r("li",[t._v("Thermal → color photos")]),t._v(" "),r("li",[t._v("Photo with missing pixels → inpainted photo")])]),t._v(" "),r("h3",{attrs:{id:"_4-2-analysis-of-the-objective-function"}},[r("a",{staticClass:"header-anchor",attrs:{href:"#_4-2-analysis-of-the-objective-function"}},[t._v("#")]),t._v(" 4.2 Analysis of the objective function")]),t._v(" "),r("p",[t._v("在 final objective 中，哪部分比较重要呢？我们进行消融研究来隔离 L1 、GAN 项的影响，以比较 cGAN 和 GAN 分别作用在输入上的效果。")]),t._v(" "),r("p",[t._v("论文中的 Figure 4 显示了这些变化在 two labels → photo 问题上的定性影响：")]),t._v(" "),r("ul",[r("li",[t._v("仅L1会导致合理但模糊的结果")]),t._v(" "),r("li",[t._v("单独使用cGAN（将 λ=0）给出了更清晰的结果，但在某些应用程序上引入了视觉瑕疵")]),t._v(" "),r("li",[t._v("将两个部分加在一起（λ= 100）减少了这些伪像")])]),t._v(" "),r("p",[t._v("使用 FCN-score 量化这些在 cityscapes labels → photo 任务中的观察研究(论文 Table 1)：")]),t._v(" "),r("ul",[r("li",[t._v("基于 GAN 的目标任务得分更高，也就是说 合成的图像包含更多的可识别结构")]),t._v(" "),r("li",[t._v("在衡量输入和输出匹配度的损失函数上，cGAN 表现的要比 GAN 好的多。\n"),r("ul",[r("li",[t._v("我们测试了从 final objective 中去掉 conditioning 部分的影响，发现，此时的损失函数不会惩罚输入和输出之间的不匹配，只关心是否输出看起来很真实。检查结果表明，生成器崩溃了，不管输入的照片如何都产生了几乎完全相同的输出。")])])]),t._v(" "),r("li",[t._v("另外，添加 L1 项也会鼓励 输出尊重输入，因为 L1 损失会对正确匹配输入的 ground truth 输出与可能不正确的合成输出之间的距离进行惩罚。")]),t._v(" "),r("li",[t._v("相应地，L1 + GAN 在创建尊重输入标签图的逼真渲染时也是有效的。结合所有项，L1 + cGAN 执行的同样的好。")])]),t._v(" "),r("p",[r("strong",[t._v("Colorfulness")]),t._v(" cGANs 的一个引人注目的影响就是 它会产生 sharp images，幻想出的即使在输入标签图中不存在的空间结构。有人就开始想象 cGANs 在光谱纬度也有类似的“锐化”作用，例如，使得图像具有更加丰富的色彩。就像 L1 在不能精确定位边缘的位置时会激发模糊一样，它也会在不确定像素应采用几种可能的颜色值中的哪种时激励颜色为平均灰色。特别是，通过在可能的颜色上选择条件概率密度函数的中值，可以使L1最小。另一方面，对抗性损失原则上可以变成能够意识到灰色输出是不现实的，并鼓励匹配真实的颜色分布。")]),t._v(" "),r("h3",{attrs:{id:"_4-3-analysis-of-the-generator-architecture"}},[r("a",{staticClass:"header-anchor",attrs:{href:"#_4-3-analysis-of-the-generator-architecture"}},[t._v("#")]),t._v(" 4.3 Analysis of the generator architecture")]),t._v(" "),r("p",[t._v("U-Net 架构允许低维度信息快捷的穿越网络，这会带来更好的结果吗？通过对比 U-Net 和 encoder-decoder(简单的移除 U-Net 中的 skip connections 形成) 在 cityscape generation 任务上的表现，我们了解到：")]),t._v(" "),r("ol",[r("li",[t._v("在我们的实验中，encoder-decoder 无法学习生成逼真的图像")]),t._v(" "),r("li",[t._v("U-Net 不仅在使用 cGANs 时 优于 encoder-decoder，在使用 L1 loss 时，也有更加优越的结果")])]),t._v(" "),r("h3",{attrs:{id:"_4-4-from-pixelgans-to-patchgans-to-imagegans"}},[r("a",{staticClass:"header-anchor",attrs:{href:"#_4-4-from-pixelgans-to-patchgans-to-imagegans"}},[t._v("#")]),t._v(" 4.4 From PixelGANs to PatchGANs to ImageGANs")]),t._v(" "),r("p",[t._v('我们测试了改变鉴别器感受野的 patch size N 的各种效果，从 1×1 的 “PixelGAN” 到 286×286 的整图 "ImageGAN"。')]),t._v(" "),r("p",[t._v("PixelGAN 对空间清晰度没有特别的影响，但是可以增加结果的色彩。颜色直方图匹配是一个图像处理领域常见的问题，也许 PixelGAN 会是一个有潜力的轻量化解决方案。")]),t._v(" "),r("p",[t._v("使用 16×16 PatchGAN 足以促进锐利的输出，并获得良好的FCN得分，但也导致 tiling artifacts。70×70 PatchGAN 缓解了这些问题并获得更好的分数。缩放到超出这个范围，到完整的 286×286 ImageGAN，似乎并没有改善结果的视觉质量，并且，事实上得到了相对低的多的分数。这也许是因为 ImageGAN 比 70 × 70 PatchGAN 有更多的参数和更大的深度，从而更加难以训练造成的。")]),t._v(" "),r("p",[r("strong",[t._v("Fully-convolutional translation")]),t._v(" PatchGAN 的优势是固定 patch 大小的判别器可以适用于任意大的图像。我们也可以将生成器卷积的应用于比训练用的图像更大的图像上。我们在 map ↔ aerial 任务上测试了这种做法。我们在 512×512 的图片上测试了用 256×256 的图像训练得到的生成器，效果在 论文的 Figure 8 中可以看出效果还是不俗的。")]),t._v(" "),r("h3",{attrs:{id:"_4-5-perceptual-validation"}},[r("a",{staticClass:"header-anchor",attrs:{href:"#_4-5-perceptual-validation"}},[t._v("#")]),t._v(" 4.5 Perceptual validation")]),t._v(" "),r("p",[t._v("我们在 map↔aerial 和 grayscale→color 任务上验证了结果的 perceptual realism（感性现实主义？？？）。用志愿者来测试生成的结果，看是否能够骗过志愿者：一般情况下，我们的方法(使用 L1+cGANs)有更加优越的表现，除非别的方法是针对特定问题进行了专门设计、优化。")]),t._v(" "),r("h3",{attrs:{id:"_4-6-semantic-segmentation"}},[r("a",{staticClass:"header-anchor",attrs:{href:"#_4-6-semantic-segmentation"}},[t._v("#")]),t._v(" 4.6 Semantic segmentation")]),t._v(" "),r("p",[t._v("有条件的 GAN 似乎可以有效解决输出高度细节化或摄影类的问题，这在图像处理和图形任务中很常见。那么在视觉问题（如语义分割）上的表现如何呢？这类问题的特点是：输入比输出更复杂。")]),t._v(" "),r("p",[t._v('为了开始对此进行测试，我们训练了一个 cGAN（带/不带 L1 损失）用于 cityscape photo→labels 任务。论文 Figure 10 显示了定性结果，分类准确性的定量报告在论文 Table 6 中。有趣的是，训练出的不带 L1 损失的 cGAN 能够在合理程度的准确性上解决这个问题。据我们所知，这是 GAN 成功生成“标签”的首次演示，这些"标签"是具有连续变化且几乎离散的值，而不是“图像”。尽管 cGAN 取得了一些成功，但他们还远不是解决下面这个问题的最佳方法：如论文 Table 6 所示：仅使用 L1 回归可以获得比 cGAN 更好的分数。我们认为，对于视觉问题，目标（即预测接近 ground truth 的产出）可能不如图形任务那么含糊，像 L1 这样的重建损失就足够了')]),t._v(" "),r("h3",{attrs:{id:"_4-7-community-driven-research"}},[r("a",{staticClass:"header-anchor",attrs:{href:"#_4-7-community-driven-research"}},[t._v("#")]),t._v(" 4.7. Community-driven Research")]),t._v(" "),r("p",[t._v("自从最初发布论文和我们的 pix2pix 代码，Twitter社区，包括计算机视觉和图形从业人员以及视觉艺术家，都有成功地将我们的框架应用于各种新颖的图像到图像的翻译任务，远远超出了原始论文。尽管有些是基于 pix2pix 进行了修改，但是，他们证明了我们的方法作为 image-to-image 转换问题的一种通用的商品工具的前景。")]),t._v(" "),r("h2",{attrs:{id:"_5-conclusion"}},[r("a",{staticClass:"header-anchor",attrs:{href:"#_5-conclusion"}},[t._v("#")]),t._v(" 5. Conclusion")]),t._v(" "),r("p",[t._v("本文的结果表明，条件对抗网络是许多 image-to-image 转换任务的有前途的方法，特别是那些涉及高度结构化图形输出的任务。这些网络可以学到适应于手头的任务和数据的 loss，这就让这些网络可以适用于各种各样的设置。")]),t._v(" "),r("hr"),t._v(" "),r("h1",{attrs:{id:"deconvolution-and-checkerboard-artifacts"}},[r("a",{staticClass:"header-anchor",attrs:{href:"#deconvolution-and-checkerboard-artifacts"}},[t._v("#")]),t._v(" "),r("a",{attrs:{href:"https://distill.pub/2016/deconv-checkerboard/",target:"_blank",rel:"noopener noreferrer"}},[t._v("Deconvolution and Checkerboard Artifacts"),r("OutboundLink")],1)]),t._v(" "),r("blockquote",[r("p",[t._v("paper: "),r("a",{attrs:{href:"./rsc/Deconvolution_and_Checkerboard_Artifacts.pdf"}},[t._v("Deconvolution and Checkerboard Artifacts")])])]),t._v(" "),r("h2",{attrs:{id:"abstract-2"}},[r("a",{staticClass:"header-anchor",attrs:{href:"#abstract-2"}},[t._v("#")]),t._v(" Abstract")]),t._v(" "),r("p",[t._v("当我们非常仔细地观察由神经网络生成的图像时，我们经常会看到一种奇怪的棋盘状伪像图案。这种现象在一些模型中尤为明显，但是最近的模型中有很大一部分都表现出了这种行为。")]),t._v(" "),r("p",[t._v("难以理解的是，棋盘格图案在色彩浓烈的图像中往往最为突出。这是怎么回事？神经网络讨厌鲜艳的色彩吗？这些伪像的实际原因实际上非常简单，这是避免它们的方法。")]),t._v(" "),r("h2",{attrs:{id:"deconvolution-overlap"}},[r("a",{staticClass:"header-anchor",attrs:{href:"#deconvolution-overlap"}},[t._v("#")]),t._v(" Deconvolution & Overlap")]),t._v(" "),r("p",[t._v("当我们让神经网络生成图像时，我们通常会根据低分辨率，高水平的描述来构建它们。这使网络可以描述粗糙图像，然后填写细节信息。")]),t._v(" "),r("p",[t._v("为了做到这一点，我们需要某种方法将低分辨率的图像转换成高分辨率的图像。通常，我们通过 Deconvolution 操作来执行此操作。粗略地讲，Deconvolution 层允许模型使用小图像中的每个点来“绘制”较大图像中的正方形。")]),t._v(" "),r("p",[t._v("（反卷积具有多种解释和不同的名称，包括“转置卷积”。为简洁起见，我们在本文中使用“反卷积”这个名称。）")]),t._v(" "),r("p",[t._v("不幸的是，反卷积很容易出现“不均匀的重叠”，在某些地方比在其他地方放置更多的隐喻绘画。特别是，当内核大小（输出窗口大小）无法被步幅（顶部各点之间的间距）整除时，反卷积具有不均匀的重叠。尽管原则上网络可以仔细地学习权重来避免这种情况（我们将在后面详细讨论），但实际上神经网络却在努力避免这种情况。")]),t._v(" "),r("p",[t._v("重叠图案也以二维形式形成。两根轴上的不均匀重叠部分会相互叠加，从而形成一个变化幅度大的棋盘状特征图案。")]),t._v(" "),r("p",[t._v("实际上，不均匀的重叠在二维上往往更加极端！因为两个图案相乘在一起，所以不均匀度平方。例如，在一维中，步幅2，大小3的反卷积具有一些输出，其输入数量是其他输入的两倍，但是在二维中，这变成四分之一。")]),t._v(" "),r("p",[t._v("现在，神经网络在创建图像时通常使用多层反卷积，从一系列较低分辨率的描述中迭代构建较大的图像。这些堆叠的反卷积可能会消除伪影，但它们通常会复合，从而在各种规模上创建伪影。")]),t._v(" "),r("p",[t._v("步幅1反卷积-在成功模型中通常被视为最后一层 - 在抑制伪像方面非常有效。他们可以删除划分其大小的频率伪像，并减少其他频率小于其大小的伪像。但是，正如许多最新模型中所看到的那样，伪影仍可能泄漏出去。")]),t._v(" "),r("p",[t._v("除了我们上面观察到的类似于高频棋盘格的伪像外，早期的反卷积还可以创建低频伪像，我们将在后面对此进行详细介绍。")]),t._v(" "),r("p",[t._v("当输出不寻常的颜色时，这些伪像往往最突出。由于神经网络层通常会有偏差（将学习值添加到输出中），因此很容易输出平均颜色。颜色（如亮红色）与平均颜色的距离越远，反卷积就需要贡献越多。")]),t._v(" "),r("h2",{attrs:{id:"overlap-learning"}},[r("a",{staticClass:"header-anchor",attrs:{href:"#overlap-learning"}},[t._v("#")]),t._v(" Overlap & Learning")]),t._v(" "),r("p",[t._v("用不均匀的重叠来思考事物是一种虽然简单化但有用的框架方法。不管好坏，我们的模型都为它们的反卷积学习权重。")]),t._v(" "),r("p",[t._v("从理论上讲，我们的模型可以学习仔细地写到不均匀重叠的位置，从而使输出均匀平衡。")]),t._v(" "),r("p",[t._v("这是一项棘手的平衡操作，尤其是当有多个通道交互时。避免伪像会显着限制可能的过滤器，从而牺牲模型容量。在实践中，神经网络努力学习完全避免这些模式。")]),t._v(" "),r("p",[t._v("实际上，不仅重叠不均匀的模型不能避免这种情况，而且重叠均匀的模型经常学习到导致相似伪像的内核！尽管这不是均匀重叠方式的默认行为，但即使重叠反卷积也仍然很容易会导致伪像。")]),t._v(" "),r("p",[t._v("完全避免伪像仍然是对过滤器的重要限制，并且在实践中，伪像仍然存在于这些模型中，尽管它们看起来较为温和。")]),t._v(" "),r("p",[t._v("这里可能有很多因素在起作用。例如，在生成对抗网络（GAN）的情况下，一个问题可能是鉴别器及其梯度（我们将在后面再讨论）。但是问题的很大一部分似乎是反卷积。最好说，反卷积是脆弱的，因为即使很仔细地选择大小，反卷积也很容易代表伪像创建函数。最糟糕的是，创建伪像是反卷积的默认行为。")]),t._v(" "),r("p",[t._v("是否有其他方法可以对伪像具有更高的抵抗力？")]),t._v(" "),r("h2",{attrs:{id:"better-upsampling"}},[r("a",{staticClass:"header-anchor",attrs:{href:"#better-upsampling"}},[t._v("#")]),t._v(" Better Upsampling")]),t._v(" "),r("p",[t._v("为避免这些伪像，我们希望使用 regular deconvolution（“转置卷积”）的替代方法。与反卷积不同，这种向上采样的方法不应将伪像作为其默认行为。理想情况下，它会走得更远，并且偏向于此类工件。")]),t._v(" "),r("p",[t._v("一种方法是确保您使用的内核大小被步幅除尽，从而避免了重叠问题。这等效于“亚像素卷积”，该技术最近在图像超分辨率方面获得了成功。但是，尽管这种方法有所帮助，但是反卷积仍然很容易陷入创建伪像的过程中。")]),t._v(" "),r("p",[t._v("另一种方法是从卷积到计算特征中分离出较高分辨率的上采样。例如，您可以调整图像大小（使用最近邻插值或双线性插值），然后进行卷积层。这似乎是一种自然的方法，并且大致类似的方法在图像超分辨率中效果很好。")]),t._v(" "),r("p",[t._v("解卷积和不同的大小调整卷积方法都是线性运算，可以解释为矩阵。这是查看它们之间差异的有用方法。反卷积在每个输出窗口都有唯一的条目，而resize-convolution则隐含了权重绑定，以阻止高频伪像。")]),t._v(" "),r("p",[t._v("我们曾经使用最近邻插值法取得了最好的结果，同时在使双线性调整大小的工作中遇到了困难。这可能仅意味着，对于我们的模型，最近邻居恰好与针对反卷积优化的超参数配合使用。这可能指出了天真的使用双线性插值可能会遇到棘手的问题，因为它过于强烈地抵抗了高频图像特征。我们不认为这两种方法都是上采样的最终解决方案，但它们确实可以修复棋盘状工件。")]),t._v(" "),r("h2",{attrs:{id:"image-generation-results"}},[r("a",{staticClass:"header-anchor",attrs:{href:"#image-generation-results"}},[t._v("#")]),t._v(" Image Generation Results")]),t._v(" "),r("p",[t._v("我们的经验是，在多种情况下，最近邻调整大小后再进行卷积效果非常好。我们发现这种方法可为您提供帮助的一个例子是“生成对抗网络”。简单地将标准反卷积层切换为最近邻调整大小，然后再进行卷积，将导致不同频率的伪像消失。")]),t._v(" "),r("p",[t._v("事实上，在进行任何训练之前就可以看到伪像的差异。如果我们使用随机权重对其进行了初始化，并查看生成器生成的图像，那么我们已经可以看到这些伪像：")]),t._v(" "),r("p",[t._v("这表明伪像是由于这种生成图像的方法，而不是对抗训练。（这也表明我们可以在没有慢速训练模型反馈周期的情况下，学习到很多好的生成器设计。）")]),t._v(" "),r("p",[t._v("相信这些伪影不是GAN特有的另一个原因是，我们在其他类型的模型中看到了它们，并且发现当我们切换到调整大小卷积上采样时，它们也会消失。例如，考虑实时艺术风格转移，其中训练神经网络直接生成样式转移的图像。我们发现它们很容易受到棋盘伪像的影响（尤其是在成本并未明确抵制它们的情况下）。但是，将反卷积层切换为调整大小卷积层会使伪像消失。")]),t._v(" "),r("p",[t._v("Google Brain团队即将发表的论文将在更全面的实验和最新结果中证明该技术的优势。 （我们选择单独介绍此技术，是因为我们认为它值得进行更详细的讨论，并且涉及多篇论文。）")]),t._v(" "),r("h2",{attrs:{id:"artifacts-in-gradients"}},[r("a",{staticClass:"header-anchor",attrs:{href:"#artifacts-in-gradients"}},[t._v("#")]),t._v(" Artifacts in Gradients")]),t._v(" "),r("p",[t._v("每当我们计算卷积层的梯度时，我们都会在向后遍历上进行反卷积（transposed convolution）。就像我们使用反卷积生成图像时一样，这会导致渐变中的棋盘格图案。")]),t._v(" "),r("p",[t._v("特征可视化社区已经知道图像模型梯度中存在高频“噪声”，这是一个重大挑战。以某种方式，特征可视化方法必须补偿这种噪声。")]),t._v(" "),r("p",[t._v("例如，DeepDream 似乎以多种方式引起伪影之间的破坏性干扰，例如同时优化许多功能，以及在许多偏移和比例上进行优化。特别是，在不同偏移量处进行优化的“抖动”消除了一些棋盘伪像。\n（尽管某些伪像是我们的标准棋盘格图案，而其他伪像是组织性较差的高频图案。我们认为，这些是由最大合并引起的。最大合并以前与"),r("a",{attrs:{href:""}},[t._v("Geodesics of learned representations")]),t._v("中的高频伪像相关联。）")]),t._v(" "),r("p",[t._v("特征可视化的最新工作（例如"),r("a",{attrs:{href:""}},[t._v("DeepDreaming with TensorFlow")]),t._v("）已明确识别并补偿了这些高频梯度分量。人们想知道，更好的神经网络架构是否可以使这些工作变得不必要。")]),t._v(" "),r("p",[t._v("这些梯度伪影会影响GAN吗？如果梯度伪影可以影响基于特征可视化中的神经网络梯度优化的图像，那么我们也可以预期它会影响由生成器参数化的图像族，因为它们由GAN中的鉴别器进行了优化。我们发现在某些情况下确实会发生这种情况。当生成器既不偏斜也不偏斜棋盘图案时，鉴别器中的大卷积会导致它们。")]),t._v(" "),r("p",[t._v("目前尚不清楚这些梯度伪像的广泛含义是什么。考虑它们的一种方法是，某些神经元基本上可以任意地获得其邻居的许多倍的梯度。等效地，出于充分的理由，网络将比其他像素更关心输入中的某些像素。这些听起来都不是理想的。")]),t._v(" "),r("p",[t._v("似乎有些像素比其他像素对网络输出的影响更大，可能会夸大对抗性反例。由于导数集中在少量像素上，因此这些像素的较小摄动可能会产生较大的影响。我们尚未对此进行调查。")]),t._v(" "),r("h2",{attrs:{id:"conclusion"}},[r("a",{staticClass:"header-anchor",attrs:{href:"#conclusion"}},[t._v("#")]),t._v(" Conclusion")]),t._v(" "),r("p",[t._v("使用反卷积生成图像的标准方法尽管取得了成功但存在一些理论上简单的问题，这些问题会导致生成的图像中出现伪像。使用没有这些问题的自然替代方法会导致伪像消失（类似的论点表明，standard strided convolutional 层也可能存在问题）。")])])}),[],!1,null,null,null);a.default=i.exports}}]);