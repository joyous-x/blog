---
title: PCA原理推导
date: 2020-04-13 00:00:00
description: PCA原理推导
categories: 
  - bigdata
tags: 
  - 
permalink:
---

# PCA原理推导

## 理论基础

> *1、矩阵相乘的物理解释*

两个矩阵相乘的意义是将右边矩阵中的每一列列向量变换到左边矩阵中每一行行向量为基所表示的空间中去。

> *2、协方差的意义*

N维向量中的两个纬度 a、b 的协方差表示两个纬度上的相关性。相关性越大，协方差越大; 协方差为0表示两个纬度完全独立。

> *3、一般的, 向量A的表示方法*
$$
A=\left\{\begin{matrix}
    a_1 \\
    a_2 \\
    \cdots \\
    a_n
\end{matrix}
\right\}
$$

$$
A^T=\left\{\begin{matrix}
    a_1 & a_2& \cdots & a_n
\end{matrix}
\right\}
$$

> *4、实对称矩阵的性质(实对称矩阵对角化)*

一个n行n列的实对称矩阵一定可以找到n个单位正交特征向量，设这n个特征向量为：
$$ e_1,e_2,\cdots,e_n $$
将其按列组成矩阵：
$$
E=\left\{
\begin{matrix}
   e_1&e_2&\cdots&e_n
\end{matrix}
\right\}
$$
则对其协方差矩阵C有如下结论：
$$
E^TCE=\Lambda=\left\{
\begin{matrix}
   \lambda_1 &&&  \\
    & \lambda_1&& \\
    &&\cdots&     \\
    &&&\lambda_n
\end{matrix}
\right\}
$$
其对角元素为个特征向量对应的特征值。


## 样本预处理

> *样本归一化*

* 每个维度都减去维度上所有样本的平均值
* 降维后，样本的方差越大保留的原始信息越多 

## 相关公式推导：
A. 由矩阵的方差公式：
$$ Var(a)=\frac{1}{m}\sum_{i=1}^{m}(a_i-\mu)^2 $$
对于归一化后的X矩阵，得到的Y的每行(每个维度)的均值依旧为0所以有：
$$ Var(a)=\frac{1}{m}\sum_{i=1}^{m}(a_i)^2 $$

B. 由矩阵的协方差公式：
$$ Var(a)=\frac{1}{m}\sum_{i=1}^{m}(a_i-\mu)(b_i-\mu) $$
对于归一化后的X矩阵，得到的Y的每行(每个维度)的均值依旧为0所以有：
$$ Var(a)=\frac{1}{m}\sum_{i=1}^{m}(a_ib_i) $$

## 降维目标 
将一组N维向量降为K维(0<K<N), 其目标是选择K个单位正交基, 使得原始数据变换到这组基上后, 各字段两两间协方差为0, 而字段内的方差则尽可能大(在正交的约束下，取最大的K个方差)

> **PCA 归一化后的特性之一 : 每个维度的均值为0**


## 推导过程
>***概述:***
将一组n维向量降k维(0<k<n), 其目标是选择k个单位正交基, 使得原始数据变换到这组基上后, 各字段两两间协方差为0, 而字段内的方差则尽可能大(在正交的约束下，取最大的k个方差)

有m个n维的样本集X，如下：
$$
X=\left\{
\begin{matrix}
   A_1 & A_2 \cdots A_m
\end{matrix}
\right\}
$$

有k个n维单位向量表示的向量空间P，如下：
$$
P=\left\{\begin{matrix}
    P_1^T \\
    P_2^T \\
    \cdots \\
    P_k^T
\end{matrix}
\right\}
$$

要将X降维，即在当 0<k<n 时，存在P，使得：
$$
Y=PX
$$
即：
$$
Y=\left\{
\begin{matrix}
   Y_1 & Y_2 \cdots Y_m
\end{matrix}
\right\}
$$

其中每一项都是k维的向量。
```
此时，我们可知：
    单位向量空间矩阵 P 为 ：k x n
    原始样本数据矩阵 X 为 ：n x m
    转换后的数据矩阵 Y 为 ：k x m
```

由理论基础可知：
1. 为了使Y尽可能多的保留X中的信息，就要使降维后的样本离散程度最大，即要使得Y的方差最大  
2. 通过方差最大化，可以选择出第一个维度，后续的维度选择依然要保留尽可能多的原始信息，所以不希望两个纬度间存在相关性，即每个维度的协方差均值为0。

即，最终要达到的目的与字段内方差以及字段间协方差有密切关系，因此我们希望能将两者统一表示。仔细观察发现，两者均可以表示为内积形式，而内积与矩阵相乘密切相关。

先看下述公式得到的矩阵C：
$$
C=\frac{1}{m}XX^T
$$
如果X是归一化后的矩阵，矩阵C有以下性质(最后两个结论需要结合‘相关公式推导A和B’)：
- 是矩阵X的协方差矩阵
- 是对称矩阵
- 对角线分别为各个维度的方差
- 第i行j列和j行i列的元素相同，表示i和j两个字段的协方差

由此可知，为了达到上述的降维目的，即需要矩阵Y的协方差矩阵对角化：除对角线外的其它元素化为0，并且将在对角线上的元素按大小从上到下排列。

接下来，我们看下原矩阵与基变换后矩阵的协方差矩阵的关系:设原始数据矩阵X对应的协方差矩阵为C，而P是一组基按行组成的矩阵，设 Y=PX，则Y为X对E做基变换后的数据。设Y的协方差矩阵为D，则有以下推导：
$$
D=\frac{1}{m}YY^T  
 =\frac{1}{m}(PX)(PX)^T  
 =\frac{1}{m}PXX^TP^T
 =P(\frac{1}{m}XX^T)P^T
 =PCP^T
$$
所以，可以发现我们要找的就是能让原始协方差矩阵对角化的P。

根据理论基础4(实对称矩阵的性质)：
$$
E^TCE=\Lambda
$$
找到我们需要的矩阵P：
$$
P=E^T
$$
P是协方差矩阵的特征向量单位化后按行排列出的矩阵，其中每一行都是C的一个特征向量。如果将P按照
$$ \Lambda $$
中特征值的从大到小，将特征向量从上到下排列，则用P的前k行组成的k*n维矩阵乘以原始数据矩阵X，就得到我们需要的降维后的数据矩阵Y。


> 参考
http://www.360doc.com/content/13/1124/02/9482_331688889.shtml